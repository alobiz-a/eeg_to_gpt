{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distinguishing between left and right sides\n",
    "\n",
    "Based on the page: https://towardsdatascience.com/i-built-a-brain-computer-interface-to-play-space-invaders-using-thoughts-23980cb4faf7 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace with our method of integrating to the Python API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REPLACE WITH PYTHON API COLLECTION METHOD!\n",
    "\"\"\"\n",
    "from pylsl import StreamInlet, resolve_stream\n",
    "import pandas as pd\n",
    "\n",
    "# initialize the streaming layer\n",
    "finished = False\n",
    "streams = resolve_stream()\n",
    "inlet = StreamInlet(streams[0])\n",
    "\n",
    "# initialize the colomns of your data and your dictionary to capture the data.\n",
    "columns=['Time','FZ', 'C3', 'CZ', 'C4', 'PZ', 'PO7', 'OZ', 'PO8','AccX','AccY','AccZ',\n",
    "'Gyro1','Gyro2','Gyro3', 'Battery','Counter','Validation']\n",
    "data_dict = dict((k, []) for k in columns)\n",
    "\n",
    "while not finished:\n",
    "   # get the streamed data. Columns of sample are equal to the columns variable, only the first element being timestamp\n",
    "   # concatenate timestamp and data in 1 list\n",
    "   data, timestamp = inlet.pull_sample()\n",
    "   all_data = [timestamp] + data\n",
    "   \n",
    "   # updating data dictionary with newly transmitted samples   \n",
    "   i = 0\n",
    "   for key in list(data.keys()):\n",
    "      data_dict[key].append(all_data[i])\n",
    "      i = i + 1\n",
    "   \n",
    "   # data is collected at 250 Hz. Let's stop data collection after 60 seconds. Meaning we stop when we collected 250*60 samples.\n",
    "   if len(data_dict['Time']) >= 250*60:\n",
    "      finished = True\n",
    "\n",
    "# lastly, we can save our data to a CSV format.\n",
    "data_df = pd.DataFrame.from_dict(data_dict)\n",
    "data_df.to_csv('EEGdata.csv', index = False) \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing:\n",
    " notch filter, common average referencing & artifact detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal, stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Having our EEG data in a Pandas DataFrame as (timepoints, channels)\n",
    "\n",
    "for curr_segment in segments:    \n",
    "    # 1. notch filter\n",
    "    b_notch, a_notch = signal.iirnotch(50, 30, sampling_frequency)\n",
    "    for column in curr_segment.columns:\n",
    "        curr_segment.loc[:,column] = signal.filtfilt(b_notch, a_notch, curr_segment.loc[:,column])\n",
    "    \n",
    "    # for next steps data should be in (channels, timepoints format)\n",
    "    curr_segment = curr_segment.T\n",
    "\n",
    "    # 2 OUTLIER DETECTION\n",
    "    for i, j in curr_segment.iterrows():\n",
    "        if stats.kurtosis(j) > 4*np.std(j) or (abs(j - np.mean(j)) > 125).any():\n",
    "            if stats.kurtosis(j) > 4*np.std(j):\n",
    "                print('due to kurtosis')\n",
    "            outlier +=1\n",
    "\n",
    "    # 3 APPLY COMMON AVERAGE REFERENCE (CAR)   \n",
    "    if  'deep' in pipeline_type: \n",
    "        curr_segment -= curr_segment.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequency filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE SURE DATA IS IN A VARIABLE NAMED 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal \n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# We want to apply a bandpass between 4 and 40 Hz\n",
    "cutoffs = [4, 40]\n",
    "# the sample frequency of our device is 250Hz\n",
    "fs = 250\n",
    "\n",
    "# we have to device the order of the filter.\n",
    "# the order determines the sharpness of the filter, i.e., around the # cutoff, do we allow some of the nearby frequency through (lower \n",
    "# order) or do we want to have a sharp cutoff (high order)? A higher \n",
    "# order introduces a bigger effect on the signal when initializing, \n",
    "# thereby destroying the original signal at that part. Note that \n",
    "# later we apply forward and backward filtering using \n",
    "# signal.filtfilt, thereby essentially doubling the order we \n",
    "# determine below.\n",
    "order = 2\n",
    "\n",
    "# initialize the filter coefficients\n",
    "b, a = signal.butter(order, [cuttoff[0]/(fs/2), cuttoff[1]/(fs/2)], \"bandpass\")\n",
    "\n",
    "# then apply the filter for each electrode channel:\n",
    "for column in data.columns:\n",
    "   data.loc[:,column] = signal.filtfilt(b, a, data.loc[:,column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# added by Ari\n",
    "#TODO: split precollected, labelled data into X_train and y_train\n",
    "\n",
    "#e.g. \n",
    "#from sklearn.model_selection import train_test_split\n",
    "#X_train, X_val, y_train, y_val = train_test_split(features, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from mne.decoding import CSP\n",
    "from pyriemann.estimation import Covariances\n",
    "from pyriemann.tangentspace import TangentSpace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization of the pipeline, fit, and predict. \n",
    "# 2 possible methods\n",
    "\n",
    "# Method 1: CSP + LDA\n",
    "csp = Pipeline(steps=[('csp', CSP()),('lda', LDA())])\n",
    "csp.fit(X_train, y_train)\n",
    "preds = csp.predict(X_val)\n",
    "\n",
    "# Method 2: Covariance + Tangent Space + Random Forest\n",
    "rg = Pipeline(steps=[('cov', Covariances(\"oas\")),('tg', TangentSpace(metric=\"riemann\")),('rf', RFC())])\n",
    "rg.fit(X_train, y_train)\n",
    "preds = rg.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# choose one of the 2 preds above, whichever works best to give preds\n",
    "\n",
    "prediction = preds[0]\n",
    "direction = ''\n",
    "while playing_game:\n",
    "  # collect data in segments of 2 seconds (using PyLSL)\n",
    "  # pre_process (notch, artifact detection, CAR) \n",
    "  # feature_extraction (frequency and spatial filtering), \n",
    "  # get_prediction (from trained model)\n",
    "  # 0 is relax, 1 is right arm motor imagery, 2 is left arm motor imagery.\n",
    "\n",
    "if prediction[0] == 0:\n",
    "    key = Key.up \n",
    "    keyboard_game.press(key)\n",
    "if prediction[0] == 1:\n",
    "    key = Key.right \n",
    "    keyboard_game.press(key)\n",
    "elif prediction[0] == 2:\n",
    "    key = Key.left\n",
    "    keyboard_game.press(key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brain_hack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
